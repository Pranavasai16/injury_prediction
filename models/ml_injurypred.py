# -*- coding: utf-8 -*-
"""Copy of ML_injurypred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12iVFdaUXz9tOgWq-3ALYrEvHq8bpmg4e
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, label_binarize
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

# Load dataset
file_path = '/content/injury_prediction_dataset.csv'
data = pd.read_csv(file_path)

# Encode categorical columns
categorical_columns = ['WeatherCondition']
encoders = {col: LabelEncoder() for col in categorical_columns}

for col in categorical_columns:
    data[col] = encoders[col].fit_transform(data[col])

# Separate features (X) and target (y)
X = data.drop(['Injury', 'DateTime'], axis=1)  # Drop target and non-numeric columns
y = data['Injury']

# Binarize the output for ROC curve
y_bin = label_binarize(y, classes=np.unique(y))
n_classes = y_bin.shape[1]

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "SVM": SVC(probability=True),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(eval_metric='mlogloss'),
    "LightGBM": LGBMClassifier(),
    "Neural Network": MLPClassifier(max_iter=1000)
}

# Apply each model
for name, model in models.items():
    print(f"\n{name}:\n{'-' * len(name)}")
    # Train the model
    model.fit(X_train, y_train)

    # Predict on test set
    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test) if hasattr(model, "predict_proba") else None

    # Calculate and display accuracy
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")

    # Display classification report
    report = classification_report(y_test, y_pred, target_names=["No Injury", "Injury"])
    print("\nClassification Report:")
    print(report)

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Injury", "Injury"])
    disp.plot(cmap=plt.cm.Blues)
    plt.title(f"Confusion Matrix - {name}")
    plt.show()

    # Bar graph for prediction count
    plt.figure(figsize=(8, 6))
    unique, counts = np.unique(y_pred, return_counts=True)
    sns.barplot(x=unique, y=counts, hue=unique, dodge=False, palette="viridis", legend=False)
    plt.title(f"Prediction Count - {name}")
    plt.xlabel("Predicted Classes")
    plt.ylabel("Count")
    plt.show()

    # ROC Curve
    if y_pred_prob is not None:
        plt.figure(figsize=(8, 6))
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_bin[y_test.index, i], y_pred_prob[:, i])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f"Class {i} (AUC = {roc_auc:.2f})")
        plt.plot([0, 1], [0, 1], "k--", lw=2)
        plt.title(f"ROC Curve - {name}")
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.legend(loc="lower right")
        plt.show()